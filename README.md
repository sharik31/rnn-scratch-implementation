# RNN from Scratch using NumPy

A **vanilla Recurrent Neural Network (RNN)** implemented **from scratch using NumPy**, including:  

. Forward propagation  
. Backpropagation Through Time (BPTT)  
.Softmax output & cross-entropy loss  
.Parameter updates with gradient descent  
. Minimal training loop on a toy dataset  

This project is a simple educational implementation to understand how RNNs work internally without relying on deep learning frameworks like TensorFlow or PyTorch.  

---

## ğŸš€ Features  

- **Single-step RNN cell** with `tanh` activation  
- **Softmax output layer** for classification  
- **Cross-entropy loss**  
- **BPTT implementation** to compute gradients  
- **Tiny training loop** to verify learning works  

---

## ğŸ“‚ Project Structure 
```text
rnn-from-scratch-numpy/
â”‚
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ rnn_from_scratch.ipynb    # Notebook version (optional)
â”œâ”€â”€ rnn_from_scratch.py       # Main RNN implementation
â”œâ”€â”€ doc/                      # Handwritten implementation notes/images
â”œâ”€â”€ requirements.txt          # NumPy dependency

```




---

## ğŸ“Š Algorithm & Architecture Overview  

This project implements a **Vanilla RNN** with the following architecture:  

- **Input size (`n_x`)** â†’ 2  
- **Hidden units (`n_a`)** â†’ 4  
- **Output size (`n_y`)** â†’ 2  
- **Sequence length (`T`)** â†’ 3 timesteps  
- **Batch size (`m`)** â†’ 1  

---

### ğŸ— **Architecture**

For each timestep **t**, the RNN cell computes:  

1ï¸âƒ£ **Hidden state update**  

`a_t` = `tanh(Wax} x_t + Waa a_{t-1} + b_a)`


where:  
- `x_t` â†’ input at time **t**  
- `a_{t-1}` â†’ hidden state from previous timestep  
- `W_ax` â†’ weights for input â†’ hidden  
- `W_aa` â†’ weights for hidden â†’ hidden  
- `b_a` â†’ bias for hidden state  

2ï¸âƒ£ **Output prediction**  

`{y}_t` = `{softmax}(W_{ya} a_t + b_y)`


where:  
- `W_ya` â†’ weights from hidden â†’ output  
- `b_y` â†’ bias for output  

---

### ğŸ”„ **Forward Pass**  

- For **T=3** timesteps, the RNN runs sequentially:  
  - Takes `x_1`, computes `a_1`, predicts `y_1`  
  - Takes `x_2`, computes `a_2`, predicts `y_2`  
  - Takes `x_3`, computes `a_3`, predicts `y_3`  

The hidden state **flows through time** and carries memory of previous steps.

---

### ğŸ” **Backward Pass (BPTT)**  

- Uses **Backpropagation Through Time**  
- Gradients are computed for each timestep  
- They are **accumulated** for all time steps before updating parameters  

---

### âš™ï¸ **Parameter Shapes**

- `W_ax` â†’ (4, 2)   â†’ hidden units Ã— input size  
- `W_aa` â†’ (4, 4)   â†’ hidden units Ã— hidden units  
- `W_ya` â†’ (2, 4)   â†’ output size Ã— hidden units  
- `b_a`  â†’ (4, 1)   â†’ hidden bias  
- `b_y`  â†’ (2, 1)   â†’ output bias  

---

### ğŸ§  **Training Setup**

- **Optimizer:** Simple Gradient Descent  
- **Learning rate:** 0.1  
- **Epochs:** 10  
- **Dataset:** Tiny toy dataset (sequence length 3)  
- **Target class:** Always class `0` for all timesteps  

---

This minimal example is **educational** and shows how RNNs work internally **without any deep learning frameworks**.

---

## ğŸ›  Installation  

```bash
git clone https://github.com/sharik31/rnn-from-scratch-numpy.git
pip install -r requirements.txt



