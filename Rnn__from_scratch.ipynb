{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1075af1",
   "metadata": {},
   "source": [
    "## BUILDING RNN FROM SCRATCH :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17360ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e32eef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  \n",
    "    return e_x / np.sum(e_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32b9dd",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f006c",
   "metadata": {},
   "source": [
    "#### For one time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c49dc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_step_rnn(xt, a_prev,Wax,Waa,Wya,ba,by):\n",
    "    a_next= np.tanh(np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba)\n",
    "    yt=softmax(np.dot(Wya,a_next)+by)\n",
    "   \n",
    "    cache = (a_next, a_prev, xt, Wax,Waa,Wya,ba,by)\n",
    "\n",
    "    return a_next,yt,cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c781f",
   "metadata": {},
   "source": [
    "#### Forward propgation through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9deb4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X,a0,Wax,Waa,Wya,ba,by):\n",
    "    n_x,m,T=X.shape\n",
    "    n_y, n_a = Wya.shape\n",
    "    a=np.zeros((n_a,m,T))\n",
    "    y_pred=np.zeros((n_y,m,T))\n",
    "\n",
    "    a_next=a0\n",
    "    caches=[]\n",
    "    for t in range(T):\n",
    "        xt=X[:,:,t]\n",
    "        a_next,yt,cache=single_step_rnn(xt,a_next,Wax,Waa,Wya,ba,by)\n",
    "        a[:,:,t]=a_next\n",
    "        y_pred[:,:,t]=yt\n",
    "        caches.append(cache)\n",
    "        \n",
    "\n",
    "    caches = (caches, X)\n",
    "    \n",
    "    return a, y_pred, caches\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa57fe6",
   "metadata": {},
   "source": [
    "### Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf5af0",
   "metadata": {},
   "source": [
    "#### For one time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8400351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_step_backward(da_next,cache):\n",
    "    a_next,a_prev,xt,Wax,Waa,Wya,ba,by=cache\n",
    "\n",
    "    dtanh= da_next*(1-(a_next)**2)\n",
    "    dWax = np.dot(dtanh, xt.T)        \n",
    "    dWaa = np.dot(dtanh, a_prev.T)\n",
    "    dba  = np.sum(dtanh, axis=1, keepdims=True) \n",
    "    dxt  = np.dot(Wax.T, dtanh)    \n",
    "    da_prev = np.dot(Waa.T, dtanh) \n",
    "\n",
    "    gradients = {\n",
    "        \"dxt\": dxt,\n",
    "        \"da_prev\": da_prev,\n",
    "        \"dWax\": dWax,\n",
    "        \"dWaa\": dWaa,\n",
    "        \"dba\": dba\n",
    "    }\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078533c0",
   "metadata": {},
   "source": [
    "#### Backpropgation through Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70d42092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches): \n",
    "    (list_caches, x) = caches  \n",
    "    (a1, a0, x1,Wax,Waa,Wya,ba,by) = list_caches[0]  \n",
    "\n",
    "    n_a, m, T = da.shape       \n",
    "    n_x, m = x1.shape  \n",
    "\n",
    "    dx    = np.zeros((n_x, m, T))\n",
    "    dWax  = np.zeros((n_a, n_x))\n",
    "    dWaa  = np.zeros((n_a, n_a))\n",
    "    dba   = np.zeros((n_a, 1))\n",
    "    da0   = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "    \n",
    "        da_current = da[:, :, t] + da_prevt\n",
    "        \n",
    "        gradients = single_step_backward(da_current, list_caches[t])\n",
    "        \n",
    "        dxt     = gradients[\"dxt\"]\n",
    "        da_prevt = gradients[\"da_prev\"]\n",
    "        dWaxt   = gradients[\"dWax\"]\n",
    "        dWaat   = gradients[\"dWaa\"]\n",
    "        dbat    = gradients[\"dba\"]\n",
    "        dx[:, :, t] = dxt\n",
    "         \n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba  += dbat\n",
    "\n",
    "    da0 = da_prevt\n",
    "\n",
    "    gradients = {\n",
    "        \"dx\": dx,\n",
    "        \"da0\": da0,\n",
    "        \"dWax\": dWax,\n",
    "        \"dWaa\": dWaa,\n",
    "        \"dba\": dba\n",
    "    } \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55734902",
   "metadata": {},
   "source": [
    "### Ctegorical Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9a53787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_pred, Y_true):\n",
    "\n",
    "    m = Y_true.shape[1]\n",
    "    loss = -np.sum(Y_true * np.log(y_pred + 1e-9)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba086f6",
   "metadata": {},
   "source": [
    "### Backprop. for da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7df9df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_backward(y_pred, Y_true, caches):\n",
    "    (list_caches, X) = caches\n",
    "    T = Y_true.shape[2]\n",
    "    n_a = list_caches[0][0].shape[0]\n",
    "    m = Y_true.shape[1]\n",
    "    n_y = y_pred.shape[0]\n",
    "    \n",
    "    dZy = y_pred - Y_true   # (n_y, m, T)\n",
    "    da = np.zeros((n_a, m, T))\n",
    "    dWya = np.zeros((n_y, n_a))\n",
    "    dby = np.zeros((n_y, 1))\n",
    "    \n",
    "    for t in range(T):\n",
    "        a_next, a_prev, xt, Wax, Waa, Wya, ba, by = list_caches[t]\n",
    "        \n",
    "        da[:, :, t] = np.dot(Wya.T, dZy[:, :, t])\n",
    "        \n",
    "        dWya += np.dot(dZy[:, :, t], a_next.T)\n",
    "        dby  += np.sum(dZy[:, :, t], axis=1, keepdims=True)\n",
    "    \n",
    "    return da, dWya, dby\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8082e",
   "metadata": {},
   "source": [
    "### Intializing RNN Dimensions, Inputs, and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fabf96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n_x = 2; n_a = 4; n_y = 2; T = 3; m = 1\n",
    "\n",
    "X = np.random.randn(n_x, m, T)\n",
    "Y_true = np.zeros((n_y, m, T))\n",
    "Y_true[0, 0, :] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab74a7",
   "metadata": {},
   "source": [
    "### Initializing Weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e26bbe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wax = np.random.randn(n_a, n_x) * 0.1\n",
    "Waa = np.random.randn(n_a, n_a) * 0.1\n",
    "Wya = np.random.randn(n_y, n_a) * 0.1\n",
    "ba = np.zeros((n_a, 1))\n",
    "by = np.zeros((n_y, 1))\n",
    "a0 = np.zeros((n_a, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b89d4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, y_pred, caches = rnn_forward(X, a0, Wax, Waa, Wya, ba, by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff4e2118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 2.071756340979015\n"
     ]
    }
   ],
   "source": [
    "loss = compute_loss(y_pred, Y_true)\n",
    "print(\"Initial loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e2f9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = output_backward(y_pred, Y_true, caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "660d74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.01335254 -0.02081293 -0.01049184]]\n",
      "\n",
      " [[ 0.01610118  0.02770829  0.01326358]]]\n"
     ]
    }
   ],
   "source": [
    "print(grads['dx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312258d1",
   "metadata": {},
   "source": [
    "### Applying gradient descent on weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7828898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(Wax, Waa, Wya, ba, by, grads, dWya, dby, lr=0.01):\n",
    "    Wax -= lr * grads[\"dWax\"]\n",
    "    Waa -= lr * grads[\"dWaa\"]\n",
    "    ba  -= lr * grads[\"dba\"]\n",
    "    Wya -= lr * dWya\n",
    "    by  -= lr * dby\n",
    "    return Wax, Waa, Wya, ba, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39e7a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n_x = 2; n_a = 4; n_y = 2; T = 3; m = 1\n",
    "\n",
    "X = np.random.randn(n_x, m, T)\n",
    "Y_true = np.zeros((n_y, m, T))\n",
    "Y_true[0, 0, :] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f96cb0",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3dcda997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 2.0718\n",
      "Epoch 2/10 - Loss: 1.6340\n",
      "Epoch 3/10 - Loss: 1.3197\n",
      "Epoch 4/10 - Loss: 1.0889\n",
      "Epoch 5/10 - Loss: 0.9154\n",
      "Epoch 6/10 - Loss: 0.7818\n",
      "Epoch 7/10 - Loss: 0.6770\n",
      "Epoch 8/10 - Loss: 0.5930\n",
      "Epoch 9/10 - Loss: 0.5247\n",
      "Epoch 10/10 - Loss: 0.4683\n",
      "\n",
      "Final predictions after training:\n",
      "Softmax probs:\n",
      " [[0.87374703 0.82510455 0.86838468]\n",
      " [0.12625297 0.17489545 0.13161532]]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    a, y_pred, caches = rnn_forward(X, a0, Wax, Waa, Wya, ba, by)\n",
    "    \n",
    "    loss = compute_loss(y_pred, Y_true)\n",
    "    da, dWya, dby = output_backward(y_pred, Y_true, caches)\n",
    "    grads = rnn_backward(da, caches)\n",
    "    \n",
    "    Wax, Waa, Wya, ba, by = update_parameters(Wax, Waa, Wya, ba, by, grads, dWya, dby, lr)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nFinal predictions after training:\")\n",
    "print(\"Softmax probs:\\n\", y_pred[:,0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a32291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6db74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a8ecb9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
